%% 
%% MicroLearn: A Modular AutoML Orchestrator through Microservices
%% Academic Article following SoftwareX template
%% 

\documentclass[preprint,12pt, a4paper]{elsarticle}

\usepackage{geometry}
\geometry{
    a4paper,
    left=2cm,
    right=2cm,
    top=1.5cm,
    bottom=3cm
}
\usepackage{listings}
\usepackage{xcolor}

% Configure code highlighting
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    rulecolor=\color{black},
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    escapeinside={\%*}{*)},
    morekeywords={String, double, static, private, public, if, else, return, new, def, async, await}
}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{tablefootnote}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{multicol}
\setlength{\parindent}{0pt}


\journal{MicroLearn}

\begin{document}
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}

\begin{frontmatter} 
\title{MicroLearn: A Modular AutoML Orchestrator through Microservices Architecture}

\author[label1]{OUHMIDA Soulaimane}
\author[label1]{BOUJNAH Zakaria}
\author[label1]{TANNOUCHE Adnan}
\author[label1]{ABOUNASR Yassine}
\author[label2]{OUEDRHIRI Oumayma}
\author[label2]{TABBAA Hiba}
\author[label3]{LACHGAR Mohamed}

\address[label1]{EMSI, School of Engineering, 5IIR, Marrakech, Morocco}
\address[label2]{EMSI, School of Engineering, Computer Science Department, Marrakech, Morocco}
\address[label3]{Cadi Ayyad University, Higher Normal School, Computer Science Department, Marrakech, Morocco}

\begin{abstract}
The exploration of machine learning (ML) models often requires advanced skills and a lengthy process: data cleaning, model selection, training, evaluation, and deployment. Existing AutoML (Automated Machine Learning) solutions remain largely monolithic, difficult to customize, and not conducive to distributed experimentation. In academic and industrial contexts, it becomes crucial to dynamically compose ML pipelines to test multiple approaches, iterate rapidly, and replicate experiments. MicroLearn addresses these challenges by providing a modular, distributed AutoML platform based on microservices architecture. Each microservice encapsulates a specific stage of the data mining pipeline: data preparation, model selection, training, evaluation, hyperparameter optimization, and deployment. Users (developers or data scientists) can compose custom ML pipelines via APIs, launch parallel training on GPUs, and compare performances through an intuitive React dashboard. The platform is designed to be scalable, customizable, and reproducible, conforming to SoftwareX standards and supporting FAIR ML principles including versioning, logging, and experiment comparison.
\end{abstract}

\begin{keyword}
AutoML \sep Microservices Architecture \sep Machine Learning Pipeline \sep Distributed Training \sep Model Selection \sep Data Preparation
\end{keyword}

\end{frontmatter}

%\linenumbers

\section*{Metadata}
\begin{table}[!ht]
\centering
\begin{tabular}{|l|p{7.5cm}|p{7.5cm}|}
\hline
\textbf{Nr.} & \textbf{Code metadata description} & \textbf{Metadata} \\
\hline
C1 & Current code version & v1.0 \\
\hline
C2 & Permanent link to code/repository used for this code version & \url{https://github.com/Soulaimane07/microlearn} \\
\hline
C3 & Permanent link to Azure DevOps & \url{https://dev.azure.com/SoulaimaneOuhmida/MicroLearn} \\
\hline
C4 & Legal Code License & MIT License \\
\hline
C5 & Code versioning system used & Git \\
\hline
C6 & Software code languages, tools, and services used & Python, FastAPI, React, TypeScript, PostgreSQL, MinIO, Docker, PyTorch, scikit-learn, XGBoost, LightGBM, MLflow, Ray, Optuna \\
\hline
C7 & Compilation requirements, operating environments \& dependencies & Docker, Python 3.11+, Node.js 18+, PostgreSQL 16, MinIO \\
\hline
C8 & Link to developer documentation/manual & \url{https://github.com/Soulaimane07/microlearn/blob/main/readme.md} \\
\hline
C9 & Support email for questions & lachgar.m@gmail.com \\
\hline
\end{tabular}
\label{codeMetadata} 
\end{table}

\section{Motivation and Significance}

Machine learning has become an essential tool across industries, from healthcare and finance to agriculture and manufacturing. However, the ML development lifecycle—spanning data preparation, feature engineering, model selection, training, hyperparameter tuning, evaluation, and deployment—remains complex and time-consuming~\cite{mlLifecycle2020}. Data scientists spend approximately 80\% of their time on data preparation and preprocessing tasks, leaving limited time for actual model development and experimentation~\cite{dataPrepTime2019}.

Existing AutoML solutions such as Auto-sklearn~\cite{autosklearn2015}, H2O AutoML~\cite{h2o2019}, and Google Cloud AutoML~\cite{googleAutoML2018} have made significant progress in automating model selection and hyperparameter optimization. However, these platforms are often monolithic, making customization difficult and limiting experimentation with alternative approaches at specific pipeline stages. Furthermore, they typically lack transparency in their decision-making processes, making it challenging for practitioners to understand why certain models were selected.

MicroLearn addresses these limitations by introducing a \textbf{modular microservices-based architecture} where each pipeline stage operates as an independent, containerized service. This design offers several key advantages:

\begin{itemize}
    \item \textbf{Modularity}: Each microservice can be developed, tested, and deployed independently, allowing teams to iterate on specific components without affecting others.
    \item \textbf{Scalability}: Individual services can be scaled horizontally based on demand—for example, scaling the Trainer service during intensive training phases.
    \item \textbf{Customizability}: Users can replace or extend individual microservices with custom implementations while maintaining compatibility with the rest of the pipeline.
    \item \textbf{Reproducibility}: The containerized architecture ensures consistent execution environments, supporting experiment replication and FAIR ML principles.
    \item \textbf{Transparency}: Each microservice provides detailed logging and metrics, enabling users to understand and audit the entire ML pipeline.
\end{itemize}

The platform supports Sustainable Development Goal 9 (Industry, Innovation, and Infrastructure) by democratizing access to advanced ML capabilities and enabling rapid prototyping of ML solutions.

\section{Software Description}

MicroLearn delivers a comprehensive platform for automated machine learning through a distributed microservices architecture. The system enables users to upload datasets, automatically clean and prepare data, select appropriate ML models, train models in parallel with GPU acceleration, optimize hyperparameters, evaluate performance, and deploy production-ready models—all through a unified API and intuitive web dashboard.

\subsection{Software Architecture}

The MicroLearn platform employs a modern microservices architecture designed for scalability, modularity, and maintainability. Each component operates independently while communicating seamlessly through RESTful APIs. The architecture is fully containerized using Docker~\cite{docker}, ensuring consistent development, testing, and production environments (see Fig.~\ref{fig:architecture}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{architecture.png}
    \caption{MicroLearn System Architecture showing the interaction between microservices, storage systems, and the frontend dashboard.}
    \label{fig:architecture}
\end{figure}

\subsubsection{Architecture Overview}

The pipeline flow follows a logical progression:
\begin{verbatim}
DataPreparer → ModelSelector → Trainer → Evaluator → Deployer
                    ↑                 ↓
               HyperOpt ← Orchestrator → Dashboard
\end{verbatim}

Each microservice is designed as a stateless, containerized component that can be scaled independently. Communication between services occurs via REST APIs, with asynchronous task management supported through message queues (RabbitMQ).

\subsubsection{Microservices Description}

\begin{enumerate}
    \item \textbf{DataPreparer (Port 8000)}
    
    The DataPreparer microservice handles the complete data preparation lifecycle, transforming raw datasets into ML-ready formats.
    
    \begin{itemize}
        \item \textbf{Technologies}: Python, FastAPI, Pandas, NumPy
        \item \textbf{Storage}: PostgreSQL (metadata), MinIO (raw and processed files)
        \item \textbf{Key Endpoints}: 
            \begin{itemize}
                \item \texttt{POST /prepare} - Execute preprocessing pipeline
                \item \texttt{GET /detect} - Auto-detect data types and issues
                \item \texttt{GET /health} - Service health check
            \end{itemize}
        \item \textbf{Features}: 
            \begin{itemize}
                \item Missing value imputation (mean, median, mode, KNN)
                \item Feature scaling (StandardScaler, MinMaxScaler, RobustScaler)
                \item Categorical encoding (OneHot, Label, Target encoding)
                \item Outlier detection and handling
                \item YAML/JSON pipeline configuration support
            \end{itemize}
    \end{itemize}
    
    \item \textbf{ModelSelector (Port 8001)}
    
    The ModelSelector automatically analyzes datasets and recommends the most suitable ML algorithms based on data characteristics, problem type, and optimization metrics.
    
    \begin{itemize}
        \item \textbf{Technologies}: Python, FastAPI, scikit-learn, PyCaret
        \item \textbf{Storage}: PostgreSQL (model catalog and recommendations)
        \item \textbf{Key Endpoints}:
            \begin{itemize}
                \item \texttt{POST /select} - Analyze dataset and recommend models
                \item \texttt{GET /models} - List available model catalog
                \item \texttt{GET /models?task\_type=classification} - Filter by task
            \end{itemize}
        \item \textbf{Supported Models (20+)}:
            \begin{itemize}
                \item \textit{Ensemble}: Random Forest, XGBoost, LightGBM, AdaBoost, Gradient Boosting
                \item \textit{Linear}: Logistic/Linear Regression, Ridge, Lasso, ElasticNet
                \item \textit{Tree-based}: Decision Tree, Extra Trees
                \item \textit{SVM}: SVC, SVR with various kernels
                \item \textit{Instance-based}: K-Nearest Neighbors
                \item \textit{Probabilistic}: Naive Bayes (Gaussian, Multinomial)
                \item \textit{Clustering}: K-Means, DBSCAN, Hierarchical
                \item \textit{Neural Networks}: MLP Classifier/Regressor
            \end{itemize}
        \item \textbf{Scoring Algorithm}: Compatibility scores (0-1) based on:
            \begin{itemize}
                \item Dataset size compatibility (30\%)
                \item Task type matching (40\%)
                \item Data handling capabilities (20\%)
                \item Complexity vs. interpretability trade-off (10\%)
            \end{itemize}
    \end{itemize}
    
    \item \textbf{Trainer (Port 8002)}
    
    The Trainer microservice manages parallel and distributed model training with GPU acceleration and experiment tracking.
    
    \begin{itemize}
        \item \textbf{Technologies}: PyTorch Lightning, Ray, MLflow, FastAPI
        \item \textbf{Storage}: MinIO (trained models), PostgreSQL (job metadata)
        \item \textbf{Key Endpoints}:
            \begin{itemize}
                \item \texttt{POST /train} - Submit training job
                \item \texttt{GET /train/\{job\_id\}} - Get job status
                \item \texttt{GET /train/\{job\_id\}/progress} - Detailed progress
                \item \texttt{GET /models/\{job\_id\}/download} - Download trained model
                \item \texttt{DELETE /train/\{job\_id\}} - Cancel training job
            \end{itemize}
        \item \textbf{Features}:
            \begin{itemize}
                \item Asynchronous training job submission
                \item Automatic GPU detection and allocation
                \item Multi-GPU distributed training support
                \item Early stopping with configurable patience
                \item MLflow integration for experiment tracking
                \item Checkpoint saving and model versioning
            \end{itemize}
    \end{itemize}
    
    \item \textbf{Evaluator (Port 8003)} \textit{(In Development)}
    
    The Evaluator microservice computes performance metrics and generates comparative reports.
    
    \begin{itemize}
        \item \textbf{Technologies}: Python, scikit-learn, Plotly
        \item \textbf{Storage}: PostgreSQL (evaluation logs)
        \item \textbf{Planned Features}:
            \begin{itemize}
                \item Classification metrics: Accuracy, Precision, Recall, F1, AUC-ROC
                \item Regression metrics: RMSE, MAE, R², MAPE
                \item Confusion matrix visualization
                \item ROC and Precision-Recall curves
                \item Cross-validation reports
                \item Model comparison dashboards
            \end{itemize}
    \end{itemize}
    
    \item \textbf{HyperOpt (Port 8004)} \textit{(In Development)}
    
    The HyperOpt microservice performs automated hyperparameter optimization using Bayesian and random search strategies.
    
    \begin{itemize}
        \item \textbf{Technologies}: Optuna, Redis, FastAPI
        \item \textbf{Storage}: PostgreSQL (trial history), Redis (optimization state)
        \item \textbf{Planned Features}:
            \begin{itemize}
                \item Bayesian optimization with TPE sampler
                \item Random search and grid search
                \item Early pruning of unpromising trials
                \item Parallel trial execution
                \item Custom search space definition
                \item Integration with Trainer for automated retraining
            \end{itemize}
    \end{itemize}
    
    \item \textbf{Deployer (Port 8005)} \textit{(In Development)}
    
    The Deployer microservice packages trained models for production deployment.
    
    \begin{itemize}
        \item \textbf{Technologies}: TorchServe, Flask, Docker
        \item \textbf{Storage}: PostgreSQL (deployment metadata)
        \item \textbf{Planned Features}:
            \begin{itemize}
                \item REST API endpoint generation
                \item Batch prediction support
                \item Edge deployment containers
                \item OpenAPI documentation auto-generation
                \item Model versioning and rollback
                \item A/B testing support
            \end{itemize}
    \end{itemize}
    
    \item \textbf{Orchestrator (Port 8080)} \textit{(In Development)}
    
    The Orchestrator coordinates pipeline execution from data preparation to deployment.
    
    \begin{itemize}
        \item \textbf{Technologies}: Node.js, NATS, Redis
        \item \textbf{Planned Features}:
            \begin{itemize}
                \item YAML-based pipeline definition
                \item Asynchronous workflow execution
                \item State tracking (pending, running, completed, failed)
                \item DAG-based pipeline dependencies
                \item Webhook notifications
                \item Retry and error handling policies
            \end{itemize}
    \end{itemize}
    
    \item \textbf{Dashboard (Port 3000)}
    
    The Dashboard provides a visual interface for managing experiments, datasets, and trained models (see Fig.~\ref{fig:dashboard}).
    
    \begin{itemize}
        \item \textbf{Technologies}: React, TypeScript, D3.js, Chart.js, Vite
        \item \textbf{Key Pages}:
            \begin{itemize}
                \item \textit{Dashboard}: Overview of active pipelines, datasets, models, and success rates
                \item \textit{DataPreparer}: Upload datasets, configure preprocessing, view transformations
                \item \textit{ModelSelector}: Analyze datasets, view recommended models
                \item \textit{Trainer}: Submit and monitor training jobs
                \item \textit{Evaluator}: View model performance metrics
                \item \textit{Deployer}: Manage model deployments
                \item \textit{Datasets}: Browse and manage uploaded datasets
                \item \textit{Models}: List trained models with metrics
                \item \textit{Settings}: Configure platform preferences
            \end{itemize}
    \end{itemize}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{dashboard.png}
    \caption{MicroLearn Dashboard showing pipeline overview, active tasks, recent datasets, and trained models.}
    \label{fig:dashboard}
\end{figure}

\subsubsection{Infrastructure Components}

\begin{itemize}
    \item \textbf{PostgreSQL 16}: Relational database for metadata, job tracking, and model catalog storage.
    \item \textbf{MinIO}: S3-compatible object storage for datasets, trained models, and pipeline artifacts.
    \item \textbf{RabbitMQ}: Message broker for asynchronous task management and inter-service communication.
    \item \textbf{Docker \& Docker Compose}: Containerization and orchestration for consistent deployment.
    \item \textbf{MLflow}: Experiment tracking, model registry, and artifact management.
\end{itemize}

\subsection{Software Functionalities}

The MicroLearn platform offers comprehensive functionalities for end-to-end ML pipeline management:

\begin{enumerate}
    \item \textbf{Dataset Management}
    \begin{itemize}
        \item Upload CSV/Parquet files via web interface or API
        \item Automatic schema detection and validation
        \item Data quality assessment (missing values, outliers, imbalances)
        \item Dataset versioning and lineage tracking
        \item Secure storage in MinIO with metadata in PostgreSQL
    \end{itemize}
    
    \item \textbf{Data Preprocessing}
    \begin{itemize}
        \item Configurable preprocessing pipelines via YAML/JSON
        \item Missing value imputation with multiple strategies
        \item Feature scaling and normalization
        \item Categorical variable encoding
        \item Feature selection and dimensionality reduction
        \item Before/after data preview with transformation details
    \end{itemize}
    
    \item \textbf{Automated Model Selection}
    \begin{itemize}
        \item Automatic task type detection (classification, regression, clustering)
        \item Target column auto-detection
        \item Intelligent model recommendation based on data characteristics
        \item Support for 20+ ML algorithms across multiple categories
        \item Custom metric optimization (accuracy, F1, RMSE, MAE, R²)
    \end{itemize}
    
    \item \textbf{Parallel Model Training}
    \begin{itemize}
        \item Asynchronous training job submission
        \item GPU acceleration with automatic device allocation
        \item Multi-GPU and distributed training support
        \item Real-time progress monitoring
        \item Early stopping and learning rate scheduling
        \item Experiment tracking with MLflow integration
    \end{itemize}
    
    \item \textbf{Model Evaluation}
    \begin{itemize}
        \item Comprehensive metric calculation
        \item Visual performance reports (confusion matrices, ROC curves)
        \item Cross-validation support
        \item Model comparison across experiments
    \end{itemize}
    
    \item \textbf{Hyperparameter Optimization}
    \begin{itemize}
        \item Bayesian optimization with Optuna
        \item Parallel trial execution
        \item Early pruning of underperforming configurations
        \item Search space customization
    \end{itemize}
    
    \item \textbf{Model Deployment}
    \begin{itemize}
        \item One-click REST API deployment
        \item Containerized model packaging
        \item Version management and rollback
        \item Auto-generated documentation
    \end{itemize}
\end{enumerate}

\section{Illustrative Examples}

To demonstrate MicroLearn's practical applicability, we present several usage scenarios that showcase the platform's capabilities for automating ML workflows.

\subsection{Scenario 1: Customer Churn Prediction}

A data scientist wants to build a customer churn prediction model using historical customer data.

\begin{enumerate}
    \item \textbf{Data Preparation}: Upload \texttt{customer\_churn.csv} through the DataPreparer interface. The service automatically detects 10,000 rows with 23 columns, identifies 147 missing values across 4 columns, and applies mean imputation, StandardScaler normalization, and OneHot encoding for categorical features.
    
    \item \textbf{Model Selection}: The ModelSelector analyzes the binary classification task and recommends:
    \begin{itemize}
        \item LightGBM Classifier (score: 0.89)
        \item XGBoost Classifier (score: 0.87)
        \item Random Forest Classifier (score: 0.85)
    \end{itemize}
    
    \item \textbf{Training}: Submit training jobs for all three models with the Trainer service. GPU acceleration reduces training time from hours to minutes. MLflow tracks each experiment with hyperparameters and metrics.
    
    \item \textbf{Evaluation}: Compare model performance—LightGBM achieves the best F1 score of 0.92 on the validation set.
    
    \item \textbf{Deployment}: Deploy the best model as a REST API endpoint for real-time churn predictions.
\end{enumerate}

\subsection{Scenario 2: Sales Forecasting}

A business analyst needs to forecast quarterly sales for inventory planning.

The workflow follows similar steps with regression-specific adaptations:
\begin{itemize}
    \item ModelSelector recommends regression models: LightGBM Regressor, XGBoost Regressor, Ridge Regression
    \item HyperOpt optimizes hyperparameters using Bayesian optimization
    \item Evaluator computes RMSE, MAE, and R² metrics
    \item Final model achieves R² of 0.94 with RMSE of \$12,500
\end{itemize}

\subsection{Scenario 3: Parallel Experimentation}

A research team wants to compare multiple preprocessing strategies and model configurations.

\begin{lstlisting}[language=Python, caption=Python API usage for parallel experimentation]
import requests

BASE_URL = "http://localhost"

# 1. Prepare data with different strategies
strategies = ["mean_imputation", "knn_imputation", "drop_nulls"]
prepared_datasets = []
for strategy in strategies:
    response = requests.post(f"{BASE_URL}:8000/prepare", 
        files={"file": open("data.csv", "rb")},
        data={"imputation": strategy})
    prepared_datasets.append(response.json()["dataset_id"])

# 2. Get model recommendations for each
for data_id in prepared_datasets:
    models = requests.get(f"{BASE_URL}:8001/select",
        params={"data_id": data_id, "metric": "f1"})
    
    # 3. Train top 3 models for each dataset
    for model in models.json()["candidates"][:3]:
        requests.post(f"{BASE_URL}:8002/train", json={
            "model_id": model["model_id"],
            "data_id": data_id,
            "experiment_name": f"experiment_{data_id}_{model['model_id']}"
        })

# 4. Compare all results in MLflow dashboard
print("View results at http://localhost:5000")
\end{lstlisting}

\section{Impact}

MicroLearn's microservices architecture delivers significant impact across multiple dimensions:

\subsection{Technical Impact}

\begin{itemize}
    \item \textbf{Scalability}: Individual microservices scale independently based on workload. During intensive training phases, the Trainer service can be scaled to multiple instances with GPU resources.
    \item \textbf{Modularity}: New algorithms or preprocessing techniques can be added to individual services without affecting the rest of the pipeline.
    \item \textbf{Reproducibility}: Docker containerization ensures consistent environments across development, testing, and production, supporting experiment replication.
    \item \textbf{Maintainability}: Clear service boundaries enable independent development, testing, and deployment cycles.
\end{itemize}

\subsection{User Impact}

\begin{itemize}
    \item \textbf{Reduced Time-to-Model}: Automated preprocessing and model selection reduce development time from weeks to hours.
    \item \textbf{Accessibility}: Intuitive React dashboard enables non-experts to build and deploy ML models.
    \item \textbf{Transparency}: Detailed logging at each pipeline stage provides insights into data transformations and model decisions.
    \item \textbf{Experimentation}: Easy comparison of multiple approaches supports iterative improvement.
\end{itemize}

\subsection{Comparison with Existing Tools}

Table~\ref{tab:tools_comparison} compares MicroLearn with existing AutoML solutions.

\begin{table}[!h]
\centering
\caption{Comparative analysis of AutoML tools.}
\label{tab:tools_comparison}
\scriptsize
\begin{tabular}{|p{3cm}|c|c|c|c|c|}
\hline
\textbf{Feature} & \textbf{MicroLearn} & \textbf{Auto-sklearn} & \textbf{H2O AutoML} & \textbf{MLflow} & \textbf{Kubeflow} \\
\hline
Microservices Architecture & Yes & No & No & Partial & Yes \\
\hline
Data Preprocessing & Yes & Limited & Yes & No & Limited \\
\hline
Model Selection & Yes & Yes & Yes & No & No \\
\hline
Parallel Training & Yes & Limited & Yes & Manual & Yes \\
\hline
GPU Acceleration & Yes & No & Yes & Manual & Yes \\
\hline
Experiment Tracking & Yes (MLflow) & No & Yes & Yes & Yes \\
\hline
Web Dashboard & Yes & No & Yes & Yes & Yes \\
\hline
REST API & Yes & No & Yes & Yes & Yes \\
\hline
Docker Native & Yes & No & Partial & Yes & Yes \\
\hline
Open Source & Yes & Yes & Yes & Yes & Yes \\
\hline
Learning Curve & Low & Medium & Medium & Low & High \\
\hline
\end{tabular}
\end{table}

MicroLearn uniquely combines comprehensive AutoML capabilities with a modular, containerized architecture that enables customization at each pipeline stage while maintaining ease of use through its intuitive dashboard.

\section{Quality Assurance}

Quality assurance was conducted across all microservices using SonarQube~\cite{sonarqube} and pytest. Table~\ref{tab:qa_metrics} summarizes the results.

\begin{table}[h!]
\centering
\caption{Quality assurance metrics for MicroLearn microservices}
\label{tab:qa_metrics}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{DataPreparer} & \textbf{ModelSelector} & \textbf{Trainer} & \textbf{Dashboard} \\
\hline
Quality Gate Status & Passed & Passed & Passed & Passed \\
\hline
Reliability Rating & A & A & A & A \\
\hline
Security Rating & A & A & A & A \\
\hline
Maintainability Rating & A & A & A & A \\
\hline
Code Duplication & 1.2\% & 1.5\% & 1.8\% & 2.1\% \\
\hline
Test Coverage & 68\% & 72\% & 65\% & 45\% \\
\hline
\end{tabular}
\end{table}

All microservices passed the SonarQube Quality Gate with A ratings for reliability, security, and maintainability. Code duplication remains below 3\% across all components. Test coverage exceeds 60\% for backend services, with ongoing efforts to improve frontend testing.

Continuous Integration/Continuous Deployment (CI/CD) pipelines are configured through Jenkins with automated testing, linting, and deployment stages.

\section{Future Work}

Several enhancements are planned for future releases:

\begin{enumerate}
    \item \textbf{Complete Microservice Implementation}: Finalize the Evaluator, HyperOpt, Deployer, and Orchestrator services currently in development.
    
    \item \textbf{Extended Model Support}: Add support for deep learning architectures (CNNs, RNNs, Transformers) for specialized tasks like image classification and NLP.
    
    \item \textbf{AutoFeature Engineering}: Implement automated feature generation and selection using genetic algorithms and neural architecture search.
    
    \item \textbf{Federated Learning}: Enable privacy-preserving distributed training across multiple data sources.
    
    \item \textbf{Model Explainability}: Integrate SHAP and LIME for model interpretation and fairness auditing.
    
    \item \textbf{Kubernetes Deployment}: Provide Helm charts for production Kubernetes deployments with auto-scaling.
    
    \item \textbf{Enhanced Visualization}: Add interactive visualizations for data exploration and model debugging.
\end{enumerate}

\section{Conclusions}

MicroLearn presents a novel approach to AutoML by leveraging microservices architecture to deliver a modular, scalable, and customizable platform for automated machine learning. The system addresses key limitations of existing monolithic AutoML solutions by enabling independent development and deployment of pipeline components, supporting parallel experimentation, and providing transparency through detailed logging and experiment tracking.

The current implementation includes fully functional DataPreparer, ModelSelector, and Trainer microservices, with Evaluator, HyperOpt, Deployer, and Orchestrator services in active development. The React-based dashboard provides an intuitive interface for both technical and non-technical users.

By adhering to FAIR ML principles—supporting versioning, logging, and reproducibility—MicroLearn serves as both a practical tool for ML practitioners and a research platform for exploring automated machine learning techniques. Its open-source nature and modular design invite community contributions and extensions.

MicroLearn exemplifies how distributed systems principles can be applied to democratize access to machine learning capabilities while maintaining the flexibility required for advanced experimentation and customization.

\section*{Acknowledgments}

This project was developed as part of the 5IIR curriculum at EMSI School of Engineering, Marrakech, Morocco. We thank our advisors Prof. Oumayma Ouedrhiri, Prof. Hiba Tabbaa, and Prof. Mohamed Lachgar for their guidance and support throughout the development process.

\bibliographystyle{unsrt}
\bibliography{bibliography}

% Manual bibliography entries for when .bib file is not available
\begin{thebibliography}{99}

\bibitem{mlLifecycle2020}
Sculley, D., et al. "Hidden technical debt in machine learning systems." Advances in neural information processing systems. 2015.

\bibitem{dataPrepTime2019}
CrowdFlower. "Data Science Report." 2016. Available at: \url{https://visit.figure-eight.com/data-science-report.html}

\bibitem{autosklearn2015}
Feurer, M., et al. "Efficient and Robust Automated Machine Learning." Advances in Neural Information Processing Systems. 2015.

\bibitem{h2o2019}
H2O.ai. "H2O AutoML." Available at: \url{https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html}

\bibitem{googleAutoML2018}
Google Cloud. "Cloud AutoML." Available at: \url{https://cloud.google.com/automl}

\bibitem{docker}
Docker, Inc. "Docker: Enterprise Application Container Platform." Available at: \url{https://www.docker.com/}

\bibitem{sonarqube}
SonarSource. "SonarQube: Continuous Inspection." Available at: \url{https://www.sonarqube.org/}

\bibitem{fastapi}
Ramírez, S. "FastAPI: Modern, fast web framework for building APIs." Available at: \url{https://fastapi.tiangolo.com/}

\bibitem{pytorch}
Paszke, A., et al. "PyTorch: An Imperative Style, High-Performance Deep Learning Library." NeurIPS 2019.

\bibitem{mlflow}
Zaharia, M., et al. "Accelerating the Machine Learning Lifecycle with MLflow." IEEE Data Eng. Bull. 2018.

\bibitem{optuna}
Akiba, T., et al. "Optuna: A Next-generation Hyperparameter Optimization Framework." KDD 2019.

\bibitem{ray}
Moritz, P., et al. "Ray: A Distributed Framework for Emerging AI Applications." OSDI 2018.

\end{thebibliography}

\end{document}
